# Cloud-Native

云原生应用架构的发展是为了应对许多常见的瓶颈，这些瓶颈会减慢整体式应用程序的开发和发布速度，即避免大量工程师围绕单个代码库工作而导致的代码“合并地狱”，支持独立发布单个服务，并在组件发生故障时提供有限的调查区域（爆炸半径）。云原生方法使开发和运营人员能够快速做出与其定义的职责相关的本地化决策，并执行这些决策。 

DevOps/SRE/平台工程工作流将软件开发和运营结合在一起，以实现更快、更频繁、故障率更低的软件发布，目标是实现持续交付的原则。  

在常见的云原生应用体系结构中，应用通常编写为多个独立的微服务，每个微服务执行不同的功能。微服务在云中的容器技术中部署和运行，云是运行容器镜像的容器运行时。微服务在云中相互通信，以便共同提供应用的功能，协同工作以形成一个分布式系统。 

分布式计算的谬论说明：

- “The network is reliable.
- Latency is zero.
- Bandwidth is infinite.
- The network is secure.”

任何应用程序都可能会出现问题，但分布式应用程序比非分布式应用程序出现故障的机会更多。

## 云原生应用程序的常见挑战

让我们仔细看看云原生应用程序可能面临的一些常见挑战。

其中第一个挑战是能否找到动态部署的所有微服务。随着微服务从一个地方迁移到另一个地方，以及部署了更多的微服务实例，要随时跟踪当前部署在哪里的所有实例变得越来越困难。这一挑战通常被称为服务发现。

第二个挑战涉及安全性和可观察性。云原生应用程序本质上更加复杂。这使得它们更难保证安全，因为它们有更大的攻击面和更多的逻辑片段需要保护。这也使得监控应用程序的运行情况和了解出错时的情况变得更加困难，尤其是在应用程序调试方面。

在使用单体架构构建的应用程序中，一个组件崩溃通常会导致整个应用程序崩溃。使用基于微服务的架构可以隔离某个服务的崩溃，从而限制爆炸半径，防止整个应用程序崩溃。但是，如果处理不当，一个服务的故障很可能会连锁影响整个应用程序。服务越多，部分故障演变成系统故障的可能性就越大，从而降低可用性并增加应用程序的停机时间

# 分布式系统的弹性(resilience)

由多个微服务组成的云原生应用程序架构构成了一个分布式系统。要确保分布式系统的可用性，减少停机时间，就必须提高系统的弹性。弹性是指使用提高可用性的策略来尽可能地减少缺陷。弹性策略的例子包括load balancing、timeouts 和automatic retries、deadlines和circuit breakers。

可以通过多种方式在分布式系统中增加弹性。例如，让每个微服务的代码都调用具有恢复功能的代码库，或者让特殊的网络代理处理微服务的请求和回复。弹性的最终目标是确保特定微服务实例的故障或性能下降不会导致级联故障，从而导致整个分布式系统宕机。

## What Is Resilience(rɪˈzɪlɪəns)?

就分布式系统而言，弹性是指分布式系统能够在不利情况发生时自动适应，以继续实现其目的。

"availability" 和 "resilience "这两个词有不同的含义。可用性是我们讨论过的：分布式系统正常运行的时间百分比。弹性是指使用各种策略来提高分布式系统的可用性。

弹性的主要目标之一是防止出现这样的情况：一个微服务实例出现问题会导致更多问题，这些问题不断升级，最终导致分布式系统出现故障。这就是所谓的级联故障。

### Resilience Strategies

应用层是应用程序驻留的地方，确认它能与其他应用程序有效通信；这是人类用户（以及其他应用程序）直接与之交互的层。应用层（第 7 层）的弹性策略内置于微服务本身。组织的开发人员可以设计和编码一个应用程序，使其在降级状态下继续工作，提供重要功能，即使其他功能由于一个或多个微服务的错误、泄露或其他问题而失效。

这类功能的一个例子是流行视频流应用的推荐功能。大多数情况下，主页包含个性化推荐，但如果相关的后端组件出现故障，则会显示一系列通用推荐。这种故障不会影响您搜索和播放视频。

传输层（第 4 层）提供网络通信功能，如确保通信的可靠传输。网络层弹性策略在第 4 层工作，监控每个微服务的已部署实例的网络性能，它们将微服务的使用请求引导至最佳实例。例如，如果某个微服务实例因其所在位置发生故障（如网络中断）而停止响应请求，新的请求将自动转到该微服务的其他实例。

将云原生应用部署为分布式系统的企业应考虑网络和/或应用层面的弹性策略

- Load balancing
- Timeouts and automatic retries//超时和自动重试
- Deadlines //最后期限
- Circuit breakers //断路器

负载平衡、超时和自动重试支持分布式系统组件的冗余。最后期限和断路器有助于减少分布式系统任何部分性能下降或故障的影响。

#### Load Balancing

云原生应用程序的负载均衡可在 OSI 模型的多个层上执行。与我们刚才讨论的 resilience 策略一样，负载平衡可以在第 4 层（网络或连接层）或第 7 层（应用层）执行。

对于 Kubernetes，第 4 层负载平衡默认使用 kube-proxy 实现。它在网络连接层平衡负载。Pod IP 地址的管理和虚拟/物理网络适配器之间的流量路由是通过容器网络接口（CNI）或叠加网络（如 Calico 或 Weave Net）实现的

回到第 4 层负载均衡，假设一个网络连接每秒向一个应用程序发送一百万个请求，而另一个网络连接每秒向同一应用程序发送一个请求。负载平衡器并不知道这种差异，它只看到两个连接。如果它将第一个连接发送到一个微服务实例，将第二个连接发送到第二个微服务实例，那么它就会认为负载是平衡的。

第 7 层负载平衡基于请求本身，而不是连接。第 7 层负载平衡器可以看到连接中的请求，并将每个请求发送到最佳的微服务实例（通常使用 Kubernetes 服务，它可以提供比第 4 层负载平衡器更好的平衡）

对于云原生应用程序，负载平衡指的是在微服务的运行实例之间平衡应用程序的请求。负载平衡假定每个微服务都有一个以上的实例；每个微服务都有多个实例可以提供冗余。在可行的情况下，这些实例都是分布式的，因此如果某个服务器甚至网站出现故障，任何微服务的所有实例都不会变得不可用。

理想情况下，每个微服务都应该有足够多的实例（副本），这样即使发生故障（如站点中断），每个微服务仍有足够多的实例可用，从而使分布式系统能继续为当时需要它的所有人正常运行

Load Balancing Algorithms（ˈælgərɪðəm算法）

+ Round Robin

  循环是最简单的算法。每个微服务的实例轮流处理请求。例如，如果微服务 A 有三个实例--1、2 和 3，那么第一个请求将分配给实例 1，第二个请求将分配给实例 2，第三个请求将分配给实例 3。当每个微服务都收到一个请求后，下一个请求就会分配给实例 1，开始实例间的新一轮循环。

+ Least Request

  最少请求是一种负载平衡算法，它将新请求分配给当时待处理请求最少的微服务实例。例如，假设微服务 B 有四个实例，实例 1、2 和 3 现在各处理 10 个请求，但实例 4 只处理两个请求。根据请求最少算法，下一个请求将转到实例 4。

+ Session Affinity

  会话亲和（也称粘性会话）是一种算法，它试图将会话中的所有请求发送到相同的微服务实例。例如，如果用户 Z 正在使用一个应用程序，并向微服务 C 的实例 1 发送了一个请求，那么同一用户会话中对微服务 C 的所有其他请求都将被定向到实例 1。

这些算法有许多变种--例如，通常会在循环算法和最少请求算法中添加weighting ，使某些微服务实例比其他实例接收到更多或更少的请求。例如，你可能想让那些通常比其他微服务实例更快处理请求的微服务实例优先。

实际上，仅靠负载平衡算法往往无法提供足够的弹性。例如，它们会继续将请求发送到已经失败且不再响应请求的微服务实例。在这种情况下，添加超时和自动重试等策略就会很有帮助。

#### Timeouts and Automatic Retries

超时是任何分布式系统的一个基本概念。如果系统的一部分发出请求，而另一部分在一定时间内未能处理该请求，则该请求超时。请求者可以自动使用系统故障部分的冗余实例重试请求。不过，有时应用程序在完全超时之前会允许 X 数量的重试。

对于微服务来说，超时是在两个微服务之间建立和执行的。如果微服务 A 的实例向微服务 B 的实例发出请求，而微服务 B 的实例没有及时处理，请求就会超时。微服务 A 实例就会自动向微服务 B 的另一个实例重试请求。

超时后重试请求不能保证一定成功。例如，如果微服务 B 的所有实例都有同样的问题，向其中任何一个实例发出的请求都可能失败。但如果只有部分实例受到影响--例如一个数据中心发生故障--那么重试就有可能成功。

此外，请求不一定总是自动重试。一个常见的原因是避免意外重复已经成功的事务。假设微服务 A 向微服务 B 发出的请求已由 B 成功处理，但它向 A 发出的回复却延迟或丢失了。在某些情况下可以重新发出请求，但在其他情况下则不行。

+ 安全事务是指相同的请求会导致相同的结果。这类似于 HTTP 中的 GET 请求。GET 是一种安全事务，因为它从服务器检索数据，但不会导致服务器上的数据被更改。多次读取相同数据是安全的，因此重新发出安全事务请求应该没有问题。安全事务也称为幂等事务(idempotent[aɪ'dempətənt] transactions)。

+ 不安全事务是指同一请求会导致不同结果的事务。例如，在 HTTP 中，POST 和 PUT 请求是潜在的不安全事务，因为它们会向服务器发送数据。重复请求会导致服务器不止一次地接收该数据，并可能不止一次地处理该数据。如果事务是授权付款或下订单，你肯定不希望重复处理太多次。

#### Deadlines

除了超时，分布式系统还有所谓的分布式超时，或者更常见的截止时间（deadlines）。这涉及系统的两个以上部分。假设有四个相互依赖的微服务： A 向 B 发送请求，B 处理请求后向 C 发送自己的请求，C 处理请求后向 D 发送请求。

假设微服务 A 需要在 2.0 秒内回复其请求。有了最后期限，完成请求所需的剩余时间就会随着中间请求一起传送。这样，每个微服务就能优先处理收到的每个请求，并在联系下一个微服务时告知该微服务还剩多少时间。

#### Circuit Breakers

Timeouts and deadlines分别处理分布式系统中的每个请求和回复。断路器更多地从 "全局 "角度来看待分布式系统。如果微服务的某个实例没有回复请求，或者回复请求的速度比预期的慢，断路器就会将后续请求发送到其他实例。

断路器的工作原理是为微服务单个实例的服务质量下降或故障程度设定一个限制。当某个实例超过该限制时，断路器就会跳闸，导致微服务实例暂时停止使用。

断路器的目的是防止一个微服务实例的问题对其他微服务造成负面影响，并可能导致连锁故障。一旦问题得到解决，微服务实例就可以再次使用。

级联故障的起因往往是针对出现性能下降或故障的微服务实例进行的自动重试。假设有一个微服务实例因请求过多而导致响应缓慢。如果断路器检测到这种情况，并暂时停止向该实例发送新请求，该实例就有机会赶上请求并恢复。

但是，如果断路器不采取行动，新请求继续向实例发送，实例可能会完全瘫痪。这就迫使所有请求转向其他实例。如果这些实例的容量已经接近饱和，新的请求也可能使它们不堪重负，最终导致故障。如此循环往复，最终导致整个分布式系统瘫痪。

## 利用Libraries实施弹性策略

利用Libraries实施弹性策略
到目前为止，我们已经讨论了几种弹性策略，包括三种形式的负载平衡以及超时和自动重试、截止日期和断路器。现在是时候开始考虑如何实施这些策略了。

刚开始部署微服务时，实施弹性策略最常见的方法是让每个微服务使用一个支持一种或多种策略的标准库。Hystrix 就是一个例子，它是一个为分布式系统增加弹性功能的开源库。[Hystrix](https://github.com/Netflix/Hystrix) 调用由 Netflix 开发，直到 2018 年，它可以被包裹在微服务中依赖于对另一个微服务请求的任何调用中。弹性库的另一个例子是 Resilience4j，它主要用于 Java 的函数式编程。

通过应用程序库实施弹性策略当然可行，但并不适用于所有情况。弹性库是针对特定语言的，而微服务开发人员通常会为每个微服务使用最佳语言，因此弹性库可能不支持所有必要的语言。为了使用弹性库，开发人员可能不得不用性能不尽如人意或存在其他重大缺陷的语言编写一些微服务。

另一个问题是，依赖库意味着要为每个微服务中的每个易受影响的调用添加调用包装器。有些调用可能会被遗漏，有些封装器可能会包含错误，而让所有微服务的所有开发人员都始终如一地做事是一个挑战。此外，还存在维护问题--未来每一个开发微服务的新开发人员都必须了解调用封装器。

## 利用Proxies实施弹性策略

随着时间的推移，基于库的弹性策略实现已经被基于代理的实现所取代

一般来说，代理位于双方通信的中间，为这些通信提供某种服务。代理通常会在一定程度上隔离双方。例如，甲方向乙方发出请求，但该请求实际上是从甲方转到代理，由代理处理该请求并向乙方发送自己的请求。

在分布式系统中，代理可以在微服务实例之间实现弹性策略，基于代理的弹性的主要优点是不需要修改单个微服务即可使用特殊的库；任何微服务都可以被代理

# SERVICE MESH数据平面和控制平面

数据平面由集群中的一个或多个service proxies 组成，每个代理通常运行在应用程序的单个服务或微服务旁边。数据平面负责微服务的服务发现、弹性、可观察性和安全性。它通过直接管理和操纵流量来实现这一目标。控制平面负责定义策略并确保数据平面遵循该策略。它管理和协调数据平面，形成一个分布式系统。

控制平面和它所管理的service proxy合在一起，就是一种架构模式。服务网格就是这种模式的一种实现方式。服务网格处理应用程序在集群内的服务对服务或 "东西向 "流量。服务网格通常与Ingress controller交互，后者管理应用程序进入群集的入口流量，即 "南北向 "流量。有了服务网格，"东西向 "流量也可以加密，这也是工程师决定实施服务网格的一个重要原因--因为它具有 mTLS 功能。

我们接下来要了解的是：

- Explain what a data plane is and what a service proxy is.
- 讨论数据平面通常提供的特征类型
- 解释什么是控制平面及其与数据平面的关系
- 描述服务网格的基本概念和高级服务网格架构
- 了解Ingress controller的功能以及它与服务网格的关系。

## Data Planes

网络流量会通过代理，代理提供检查或处理网络流量的功能。在服务网格中，数据平面是一组代理，每个代理专用于应用程序的一个服务或微服务。数据平面内的每个代理都被称为service proxy（有时也称为service mesh proxy）。service proxy通常也被称为sidecar proxies，因为它们是与服务或微服务一起部署并在进程外运行的，服务或微服务并不直接控制代理。

下图是一个简化的云原生应用程序示例，它由三个微服务组成： 其中每个微服务都有一个相应的service proxy，每个微服务只在其 pod 中与其service proxy直接对话。图中service proxy和微服务之间的蓝色箭头表示了这一点。

箭头表示哪些实体可以启动通信；对于蓝色箭头，双箭头表示service proxy可以启动与其微服务的通信，而微服务可以启动与其service proxy的通信。

每个service proxy都代表自己的微服务与其他service proxy（以及间接与它们的微服务）进行通信。这些通信通常采用远程过程调用（RPC）的形式。图中的红色箭头描述了这一点：A 的service proxy与 B 的service proxy之间的通信，以及 B 的service proxy与 C 的service proxy之间的通信。

请注意，还有一些箭头源自每个service proxy并指向外部，而且这些箭头只朝一个方向。这表明service proxy具有egress capabilities。本章稍后我们将详细介绍egress功能。

橙色箭头表示service proxy也可以接受来自其他地方的incoming请求。service proxy实际上既是代理也是反向代理。我们通常认为代理首先是处理针对特定应用程序微服务的传入请求（Ingress）。不过，service proxy也会处理从其应用程序的微服务向其他微服务发出的请求(Egress)。处理传入请求的第一部分是代理。第二部分处理发出的请求，即反向代理。为了简洁起见，人们会说service proxy就是代理，但其实它既是代理也是反向代理。

在数据平面中实现功能意味着不必在每个微服务中实现这些功能。这也意味着，无论使用哪种编程语言实现每个服务，这些功能都将在整个应用程序中一致地实现。

此图仅描述了服务网格的一部分。另一个元素是控制平面--它负责管理和协调service proxy

<img src=".\img\1.png" alt="1" style="zoom:50%;" />

### Service Proxy 功能

service proxy在 Kubernetes 集群中为云原生应用程序的微服务之间提供功能。这些功能通常包括以下内容：

**Service discovery**
当一个微服务需要向另一个微服务发出请求时，第一个微服务的service proxy需要找到能处理该请求的第二个微服务的实例。在某些情况下，该实例还需要具有特定的特征或属性。在拥有大量微服务的动态分布式系统中，服务发现是一项艰巨的任务。

**Resilience**
在service proxy的微服务中，可能需要实施一些弹性策略，其中可能包括实施之前讨论的任何或所有流量管理弹性策略，如load balancing, timeouts, deadlines, and circuit breakers，以及代表service proxy的微服务实施的其他策略。弹性还包括验证所需的微服务实例是否可用，并代表service proxy的微服务向正确的地方发送请求。

**Observability [əb'zɜːvəbɪlɪtɪ] **（可观察性）
service proxy可以观察收到的每个请求和回复。它还可以收集遥测数据，如微服务的性能或健康状况。最重要的是，它可以收集有关流量本身的指标，如吞吐量、延迟和故障率。

**Security**
service proxy可以识别违反策略的请求和回复，并阻止它们通过。service proxy可提供的其他安全功能包括验证其他微服务的身份，并根据验证的身份执行访问控制策略。

这些都是云原生应用程序经常面临的共同挑战：服务发现、弹性、可观察性和安全性方面的挑战。service proxy支持解决所有这些问题的功能

## Service Mesh Data Plane Solutions

在出现基于 sidecar 的代理之前，服务代理所提供的一些与现在相同的功能已经内置于特定语言的弹性库中，之前已经讨论过。Twitter 的 Finagle、Netflix 的 Hystrix 和 Google 的 gRPC 都是通信和弹性库的典范，它们旨在为服务间的远程过程调用提供弹性。

随着时间的推移，解决方案也从弹性库转向了具有第 7 层感知（OSI 网络模型）的进程外代理。这些代理具有与库相同的 RPC 弹性功能。HAProxy 就是这些代理的一个例子，AirBnB 的 SmartStack 在 2013 年使用了 HAProxy。第一个用于成熟服务网格解决方案的第 7 层代理是 linkerd-proxy，随后不久 Envoy Proxy 也开始使用。

常见的服务网格数据平面解决方案包括 Envoy Proxy、linkerd-proxy、NGINX 和 HAProxy。我们将详细介绍两种流行的选择： Envoy Proxy 和 linkerd-proxy。

## Control Planes and Service Meshes

控制平面负责管理和协调数据平面，即集群中应用程序的所有服务代理，从而形成一个分布式系统。控制平面不会直接观察或影响任何服务请求或回复；请求和回复不会通过控制平面。

由专人定义策略并配置控制平面，然后由控制平面负责配置数据平面并监控其状态和组成数据平面的服务代理的状态。控制平面确保数据平面中的每个服务代理都遵循定义的策略。

控制平面和它所管理的数据平面结合在一起就是一种架构模式。服务网格就是这种模式的一种实现方式。服务网格位于集群内部，处理应用程序的服务到服务、东西向网络流量。我们将之后讨论控制平面/数据平面模式的另一种实现方式。

本图显示了一个简化的服务网格。控制平面直接与数据平面内的每个服务代理交互。控制平面和服务代理之间的橙色虚线表示控制通信。服务代理及其相应的微服务构成数据平面。为简单起见，本图只显示了控制通信，而未显示任何数据通信

<img src=".\img\2.png" alt="2" style="zoom:50%;" />

## external control plane

在使用服务网格时，一个重要的考虑因素是，你是希望服务网格的控制平面位于 Kubernetes 集群本地，还是外部实体。从简单的角度来看，你可以在同一个 Kubernetes 集群上运行控制平面。不过，随着部署变得越来越复杂，出于几个原因，你很可能想创建一个外部控制平面。

首先，你有多个集群需要管理。如果你的环境有 5 个、10 个或 50 个群集，你可以想象，同时管理 5 个、10 个或 50 个服务网格控制平面会变得相当麻烦。相反，你可以使用所有 Kubernetes 集群都要检查的外部控制平面来调用服务网格配置。

其次，从安全角度来看，群集之间有明确的职责分工。当基础架构的各个部分被隔离开来，你就可以对每个部分进行相应的管理，从而更容易降低安全风险。

简而言之，你想使用外部控制平面的两个最大原因是，这样你就能分清责任，并尽可能降低安全风险。

pool 中连接到external control plane的第一个远程 Kubernetes 集群将作为网格的**config cluster**。

<img src=".\img\3.png" alt="3" style="zoom:50%;" />

## Service Mesh Technologies(/**tek'nɒlədʒi**/技术)

[Envoy Proxy](https://www.envoyproxy.io/) 是服务网格用作数据平面的热门选择。Envoy Proxy 最初由 Lyft 开发，现在是云原生计算基金会的一个项目，有来自 AirBnb、亚马逊、微软、谷歌、Pinterest 和 Salesforce 等多家公司的数百名贡献者

基于 Envoy 的服务网格实例包括

- [Consul](https://www.consul.io/)
  Consul 是由 HashiCorp 管理的一个开源软件项目
- [Istio](https://istio.io/)
  Istio 是谷歌、IBM 和 Lyft 的开源项目
- [Kuma](https://kuma.io/)
  Kuma 是一种服务网格控制平面实现。Kuma 由 Kong Inc提供.

[Linkerd](https://linkerd.io/)是一种非基于 Envoy 的服务网格, 它使用自己的代理[linkerd2-proxy](https://github.com/linkerd/linkerd2-proxy). Linkerd 由云本地计算基金会 (CNCF) 托管。

## Ingress Controllers

服务网格位于群集内，处理群集内服务与服务之间的东西向流量。不过，服务网格还需要一种方法，让其微服务能够回复来自用户和群集外其他实体的请求。有了 Kubernetes，每个 pod 都有自己的 IP 地址，每个群集实际上就成了独立于互联网的网络。因此，为了将流量路由到集群内的微服务，需要一个特殊的基础架构软件。

术语 "ingress"一般指进入网络的网络流量。在 Kubernetes 的语境中，Ingress 指的是 Ingress API 对象，它负责路由和控制进入 Kubernetes 集群的入站网络流量。

在 Kubernetes 中，有一种称为 Ingress 控制器的模式来执行 Ingress 功能。Ingress 控制器管理进入集群的应用程序南北向流量。来自集群外部的流量被发送到 Ingress 控制器，它将流量路由到集群中的相应服务。

在使用服务网格时，还需要入口控制器将流量路由到服务网格。输入控制器和服务网格之间的集成分为两个层面：

+ 在控制平面层面， Ingress controller and service mesh需要共享服务发现信息，以便将流量持续路由到正确的目标目的地。
+ 在数据平面层面，输入控制器和服务网格共享加密机密，以确保所有通信的安全。

有些入口控制器还可用作 API 网关。API 网关提供的额外管理功能超过了典型的接入控制器。这种功能通常包括身份验证、速率限制和 API 管理功能。

另一种可管理进入群集的南北流量的模式称为 API 网关。有时，"Ingress 控制器 "和 "API 网关 "是同义词，但它们通常有不同的含义。入口控制器是 Kubernetes 特有的。在服务网格中，API 网关包括 Ingress 控制器。不过，API 网关还能执行其他功能，并非 Kubernetes 独有。API 网关在许多环境中都有使用，而且除了用于 Ingress 控制外，还可用于其他目的。

**在服务网格中使用的ingress术语包括提供 Ingress 控制器功能的 API 网关**

Examples of Ingress controllers include:

- [Ambassador](https://www.getambassador.io/products/api-gateway/) 是一个基于 Envoy Proxy 的 Kubernetes 原生 API 网关和 Ingress 控制器。
- [Contour](https://projectcontour.io/) 是另一个基于 Envoy Proxy 的开源 Ingress 控制器。
- [ingress-nginx](https://kubernetes.github.io/ingress-nginx/) 是一个基于 NGINX 的开源 Ingress 控制器。

## Ingress Controller and Service Mesh Control Plane

入口控制器与服务网格协同工作。下图展示了一个简化的服务网格，在此基础上，本图增加了三个重要元素：

- Ingress data plane    （数据平面的输入控制器），位于集群内部的边缘处
- Ingress control plane（控制平面的输入控制器），也位于群集内部
- external load balancer，位于群集外部，连接群集与外部世界

<img src=".\img\4.png" alt="4" style="zoom:50%;" />

**请注意:**，外部负载平衡器、输入数据平面和微服务都是灰色的，图中还有许多箭头也是灰色的。这些灰色元素用于数据通信。我们将暂时搁置这些元素，但稍后会再讨论它们。

现在，让我们把注意力集中在用虚线连接的方框上。虚线代表控制通信。如下图所示，本图显示了服务网格控制平面与数据平面中每个服务代理之间的控制通信。

本图还显示了控制网格平面与入口控制平面之间的控制通信。我们将在本课程稍后部分详细介绍这些通信，但基本上服务网格控制平面会经常向 Ingress 控制器更新其数据平面中微服务的状态。ingress控制器使用这些信息来决定如何处理从外部负载平衡器进入群集的传入请求。

<img src=".\img\5.png" alt="5" style="zoom:50%;" />

## Ingress Controller and Service Mesh 数据平面

<img src=".\img\6.png" alt="6" style="zoom: 67%;" />

 Ingress controllers还直接与服务网格数据平面交互。在本图中，控制通信以灰色显示，数据通信以蓝色箭头（仅在服务代理和各自的微服务之间）和红色箭头（所有其他数据通信）显示。

**现在假设群集外的微服务生成了一个请求，该请求的目的地是群集内的微服务：**

+ 在步骤 1 中，请求将从外部世界到达外部负载平衡器。外部负载平衡器的例子包括亚马逊弹性负载平衡器、Google HTTP(S) 负载平衡器、Azure 标准负载平衡器等云负载平衡器，以及 MetalLB 等裸机负载平衡器。
+ 外部负载平衡器将在第 2 步中把请求发送到其群集，由群集的Ingress data plane接收。
+ 输入数据平面将在步骤 3 中把请求路由到 service proxy 。
+ service proxy 将在第 4 步中接收请求并联系其微服务以处理请求。
+ 微服务的回复将沿相同路径离开群集，到达生成请求的微服务。

Ingress controllers 还可以帮助保护它们从外部世界接收的通信。例如，外部客户端可能希望验证其连接对象的身份，并确保通过连接传输的信息安全，防止攻击者窃听和篡改。用于提供这种安全性的协议称为传输层安全协议（TLS），我们将后面部分详细讨论。

Ingress controllers提供一种称为 TLS 终止的功能。这是一个向客户端提供数字证书的过程，这样客户端就可以使用 TLS 并解密 TLS 连接。由于入口控制器是第 7 层代理，因此必须执行 TLS 终止，这样才能读取传入请求的内容，并将每个请求路由到正确的位置。你将在后边的实验室中仔细了解 TLS 终止。

## Egress Requests

还有一种数据通信类型需要注意：**egress requests**。这些请求由数据平面的微服务生成，并在集群外处理。

让我们举例说明egress requests的路径。假设群集内的一个微服务生成了一个要发送给群集外微服务的请求。

<img src=".\img\7.png" alt="7" style="zoom:50%;" />

+ 在步骤 1 中，微服务会将请求发送到其服务代理。
+ 在步骤 2 中，服务代理会发现请求需要发送到群集之外，因此它会将请求从群集转发到外部负载平衡器。流量不会经过Ingress controller--入口控制器严格处理**Ingress requests**，而不是**egress requests**。
+ 在步骤 3 中，外部负载平衡器会将出口请求路由到目标群集。

# 服务网的效益和成本(BENEFITS AND COSTS)

我们将讨论服务网格的优势和成本。服务网格可为云原生应用程序带来诸多好处。服务网格技术可以帮助解决的常见挑战包括服务发现、弹性、可观察性和安全性。您将了解到服务网格的几种功能，这些功能可能对您的应用程序有益。

我们还将讨论使用服务网格的一些弊端和成本。服务网格会增加应用程序的复杂性，并通过消耗资源和扩大延迟来增加开销。服务网格技术也在快速发展，因此需要付出更多努力才能跟上。我们将为您提供一些指导，帮助您了解服务网格技术何时适合特定情况，何时不适合特定情况。

我们将：

- 描述服务网格技术的常见功能，并解释这些功能可以带来的好处
- 解释服务网格的成本以及解决这些成本的方法
- 解释什么情况下需要使用服务网状网，什么情况下只使用入口控制器就足够了

## 服务网格的功能和优势

在前面的章节中，我们谈到了云原生应用程序在服务对服务通信中面临的四个常见挑战：服务发现、弹性、可观察性和安全性。服务网格有助于应对所有这些挑战，而且它是透明的，通常无需更改应用程序的代码。

在上一章中，我们介绍了服务网格的控制平面和数据平面以及 Ingress 控制器，以及它们是如何协同工作的。我们研究了概念性的高级架构，这些架构说明了数据和控制通信如何在架构组件之间流动。

现在，我们将讨论服务网格通常提供的几种功能，并解释这些功能如何有利于企业的开发和运营。我们将按照挑战类别对这些功能进行分组：**service discovery, resilience, observability, and security**

### Service Discovery

在服务网格中，服务发现指的是查找微服务并跟踪它们在网络中位置的流程。服务发现可以通过多种方式进行，包括使用任何现有的域名系统（DNS）基础设施、现有的强一致性键值数据存储或专门的服务发现解决方案。这些方法在很大程度上超出了大多数服务网格的范围。

关于服务发现，最重要的是服务网格需要了解当前部署了哪些微服务以及如何与它们通信的最新信息。没有这些信息，服务网格就无法正常运行。

此外，有必要确保在部署每个微服务实例时，自动为其分配一个部署在其 pod 中的服务代理。微服务的高度动态性和实例数量意味着，每次部署另一个微服务实例时都要手动分配服务代理是不可行的。

这些自动服务代理分配的一个术语是自动 sidecar 注入，其中 "sidecar "指的是服务代理，而 "注入 "指的是将服务代理置于微服务实例的 pod 中。另一个术语是自动代理注入。

微服务与 pod 中的服务代理之间的通信使用的是 localhost 环回适配器。pod 中的每个容器都能访问该 pod 的 localhost 回环适配器，其 IP 地址为 127.0.0.1。。每个服务代理和微服务对都使用其 pod 中的 localhost 环回适配器进行通信。服务代理和微服务在 pod 内相互通信时，都使用 127.0.0.1 作为源地址和目标地址。

localhost 环回适配器只在单个 pod 内有效，而 127.0.0.1 地址不可路由，因此在 pod 外部通信时，每个服务代理都使用不同的可路由 IP 地址。要知道，每个 pod 都有自己的本地主机环回适配器；一个服务代理使用的 127.0.0.1 地址无法被其他任何服务代理访问，因为它们都在不同的 pod 中。

### Resilience: Timeouts and Automatic Retries

在 "分布式系统的弹性 "中，我们讨论了几个弹性概念，包括超时和自动重试。让我们看看服务网格如何支持这些概念。

超时和自动重试听起来很简单，但如果配置不当，它们实际上可能会导致问题。例如，自动重试的时间可能会过长，从而使用户感到沮丧和受阻。如果某个问题导致大量自动重试同时发生，重试本身就会使系统不堪重负，导致速度更慢。

设置较低的自动重试最大次数有助于降低这一问题的严重程度，但最大次数应该是多少呢？有时，你希望一个微服务重试的次数多一些，而另一个微服务重试的次数少一些。另一个原因是，有些 pod 可能依赖于特定的启动服务，因此可能需要重试几次，这取决于 pod 中运行的容器数量。此外，在所有情况下都使用一个最大值也不太合适--有时你会同时进行多次重试，因此你可能希望降低最大值，而有时你只进行了几次重试，实际上你可能希望提高最大值。

服务网格可以通过几种方式帮助解决这些问题。最常见的方法是service profile（在 Linkerd 中）或virtual service（在 Istio 中）。ervice profile或virtual service为您提供了一种定义微服务路由的方法。一旦定义了路由，就可以为每个路由指定超时长度和最大自动重试次数。

请注意，service profiles和virtual services还可用于其他用途。稍后我们将详细介绍。

有些服务网格还提供自动重试功能，称为**retry budget**。使用retry budget时，服务网格会随时跟踪有多少首次请求和多少重试请求正在处理中。服务网格会确保首次请求与重试请求的比率保持在可接受的水平。假设比例设定为 1:10。如果有 100 个首次请求正在处理中，则服务网格同时处理的重试请求不会超过 10 个。

### Resilience: Circuit Breakers

某些服务网格技术支持断路器功能。正如我们在 "弹性 "中所讨论的，断路器的工作原理是为微服务的单个实例的服务质量下降或故障程度设置限制。当实例超过该限制时，断路器就会跳闸，导致微服务实例暂时停止使用。使用断路器可以防止小问题演变成连锁故障。

每种具有断路器功能的服务网格技术定义和实现断路器的方式都不尽相同。不过，一般来说，断路器是通过一组配置设置来定义的，这些配置设置指定了应监控的条件以及断路器应在什么情况下跳闸。

### Resilience: Load Balancing

每个服务网格至少支持一种类型的请求负载平衡。有些（如 Istio 和 Kuma）默认使用循环负载均衡，但也支持其他类型，如加权请求分配。Linkerd "使用一种称为 EWMA（exponentially weighted moving average即指数加权移动平均）的算法，自动将请求发送到最快的端点"。(Linkerd 的 "负载平衡） ([*"Load Balancing"*](https://linkerd.io/2/features/load-balancing/) by Linkerd)

负载平衡可用于多种特殊目的，以帮助development and operations。其原理是部署一个或多个微服务的多个版本或配置，然后配置服务网格，将一定比例的请求发送到每个版本或配置。这样就可以测试微服务的新代码、配置设置或其他方面，并与现有代码、设置等进行比较。

以这种方式使用负载平衡的一般术语是流量转移和流量分割。这些术语的定义和使用并不一致，因此存在相当大的混淆。大多数情况下，流量转移指的是将请求从微服务的一个版本或配置逐步转移到另一个版本或配置，以取代原来的版本或配置；而流量分割指的是将请求分给两个或多个版本或配置，以某种方式对它们进行比较。不过，这两种功能通常是结合在一起的，比如比较多个版本，然后将用户过渡到某个特定版本。

与只说 "流量转移(traffic shifting) "或 "流量分割(traffic splitting) "相比，通常更清楚地说明改变流量去向的原因。下面是一些常见原因的例子。

+ A/B 测试指的是对微服务的两个版本或配置（A 和 B）进行实验。这样做可能是出于业务原因，例如查看人们对新应用功能的反应，也可能是出于操作原因，例如查看新的微服务调整对负载下的性能有何影响。下图显示了请求如何在两个版本的微服务之间对半分配，每个版本都在尝试新的应用功能。

  <img src=".\img\ABtesting.png" alt="ABtesting" style="zoom:50%;" />

+ 蓝绿部署指的是有两个生产环境，一个用蓝色表示，另一个用绿色表示。假设用户目前正在蓝色环境中运行的应用程序中进行交互。您将在绿色环境中部署下一版应用程序，并在那里进行最终测试。

  下图描述了两个版本的应用程序如何在两个环境中同时运行。在左侧，所有请求都会进入蓝色环境。准备将用户过渡到下一版本时，所有新请求都会从蓝色环境转移到绿色环境，如右图所示。如果新版本出现问题，你可以同样快速地将新请求转回仍在运行旧版本的蓝色环境。

  <img src=".\img\bluegreen.png" alt="bluegreen" style="zoom:50%;" />

+ 金丝雀部署、金丝雀推出和金丝雀发布都是指通过向新的微服务发送一定比例的请求，并随着时间的推移逐步提高该比例，从而测试新的微服务--金丝雀。下图举例说明了发送到当前微服务和新微服务的请求所占比例会如何随时间推移而变化。当所有请求都转到新版本时，金丝雀部署结束。

  <img src=".\img\canary deploy.png" alt="canary deploy" style="zoom:50%;" />

### Resilience: Fault Injection(故障注入)

大多数服务网格技术都支持的另一项弹性功能称为故障注入。故障注入意味着您可以通过有目的地生成故障（错误）来测试您的应用程序，以了解它们对应用程序的影响。这可以很好地模拟各种情况，确保应用程序的行为符合您的预期。

在不同的服务网格技术中，用于配置故障注入的机制也大不相同。它们的共同点是灵活性。由于所有流量都会通过启动和接收微服务，因此您可以让微服务返回任何您想要的 HTTP 错误代码，或者让微服务出现请求超时。您可以根据需要随时更改故障注入，也可以随时启动和停止故障注入。

通过服务网格技术进行故障注入比更改应用程序微服务代码来模拟故障要方便得多。服务网格可以自行注入许多相同的错误条件和其他不利情况，而完全不必更改应用程序的代码。

### Observability: Telemetry and Metrics

除了服务发现和恢复能力，服务网格技术还有助于提高应用程序的可观察性。可观察性是指能够监控应用程序的状态，并在应用程序出现问题时从高层次确定发生了什么。

讨论可观察性时经常使用的两个术语是**metrics and telemetry**。遥测指的是收集数据来衡量某件事情，而度量则是这些衡量标准的另一个名称。

谷歌的工程师们提出了所谓的[Four Golden Signals](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/)。他们认为这些指标在监控分布式系统（包括云原生应用程序）时尤为重要。下面简要介绍一下微服务和服务网格技术背景下的四个 "黄金 "指标：

- **Latency**
  收到请求回复需要多长时间。
- **Traffic**
  系统面临的需求量，如每秒请求次数。
- **Errors**
  出错的请求占多大比例。
- **Saturation [ˌsætʃə'reɪʃn]饱和度
  可用资源（如处理能力或内存）的充分利用程度

服务网格技术通常支持收集这四种黄金指标中的大部分或全部指标，而且通常还支持收集其他指标。服务网格技术还提供多种访问指标的方法，例如通过图形仪表盘查看指标，以及通过 API 导出指标供其他工具使用。

### Observability: Distributed Tracing

服务网格提供可观察性的另一种方式是分布式跟踪。分布式跟踪的原理是在每个请求中添加一个特殊的跟踪头，每个头中都有一个唯一的 ID。通常情况下，这种唯一 ID 是在入口处添加的通用唯一标识符 (UUID)。这样，每个 ID 通常都与用户发起的请求相关，在故障排除时非常有用。每个请求都能被唯一识别，其在网状结构中的流向也能被监控--它在何时何地穿过网状结构。

每个服务网格都以不同的方式实现分布式跟踪，但它们有一些共同点。它们都要求修改应用程序代码，以便在每个请求中添加唯一的跟踪头，并在整个服务调用链中传播。它们还请求使用单独的跟踪后端。

分布式跟踪的目的是在服务网格已收集的指标和其他信息无法提供足够信息来排除故障或理解意外行为时使用。在使用分布式跟踪时，可以为了解服务网格内发生的情况提供有价值的信息。

所有服务网格的跟踪头都使用相同的规范: B3. 如果你感兴趣，可以在 GitHub 上了解有关 B3 跟踪头规范的更多信息 [openzipkin/b3-propagation](https://github.com/openzipkin/b3-propagation).

### Security: Mutual TLS

服务网格可以通过使用加密协议传输层安全（TLS）来保护 pod 之间的通信。TLS 使用加密技术确保通信信息不会被他人监控或篡改。例如，如果恶意行为者可以访问服务网格使用的网络，那么他就无法看到微服务与微服务通信中传输的信息。

在服务网格中，服务代理之间使用 TLS。服务网格实际上使用的是一种名为mutual  的 TLS。每个服务代理都有一个a secret cryptographic key ，用于确认其身份，并允许其对收到的通信进行解密。每个服务代理也都有一个其他服务代理的证书，允许它对发送的通信进行加密，只有通信的目标接收者才能解密。

基本上，在 pod 与 pod 之间启动任何通信时，每个服务代理都会首先通过身份验证验证其他服务代理的身份，然后对通信进行加密，以便其他服务代理无法访问。下图是这方面的一个简化示例。一个 mTLS 会话保护 A 和 B 服务代理之间的通信，另一个 mTLS 会话为 B 和 C 之间的通信提供类似的保护。由于每个微服务实例都有唯一的密钥，因此每个会话的加密方式只有特定的微服务实例才能解密。

还需要注意的是，mTLS 只保护服务代理之间的通信。它无法为 pod 内部服务代理与其微服务之间的通信提供任何保护。服务网格架构认为，如果攻击者已经可以访问 pod 内部，那么加密通信就没有意义了，因为攻击者无论如何都可以访问未加密的信息。

<img src=".\img\mtls.png" alt="mtls" style="zoom:50%;" />

mutual TLS 的另一个好处是，由于它能确认每个服务代理的身份，因此能让服务网格根据这些身份来划分网络。新的安全模式可以根据服务代理身份执行策略，而不是像以前那样根据 IP 地址执行策略。例如，你可以创建规则集，防止处理敏感信息的高信任度微服务接收低信任度微服务的请求。通过相互 TLS 验证的身份来强制执行信任边界，有助于防止攻击者通过微服务横向移动来获取最有价值的资产。

## 服务网格的缺点和成本

服务网格可以带来很多好处，但这些好处也是有代价的。服务网格有很大的缺点，在决定实施服务网格之前需要考虑这些缺点。常见的缺点包括以下几点：

**Increased complexity**(复杂性增加)
有了服务网格，架构中就有了更多的组件。这意味着可能出错的地方更多。这也意味着人们更难理解所有组件以及它们如何协同工作，因此正确配置组件和排除故障等工作将更具挑战性。

**More overhead（更多开销）**
服务网格中的额外组件意味着需要更多的计算资源和其他资源，通信延迟也可能会增加。每个服务网格用例需要额外资源和出现延迟的程度都不同。

**服务网格技术的快速发展**
服务网格技术仍然很新，而且经常变化。虽然这在某些方面是好事，如增加新功能，但也意味着这些技术不一定稳定。您可能需要有意识地跟上服务网格技术的最新发展，以便在新版本出现时做好更改配置或进行其他调整的准备。

## 何时需要服务网格？

并不是每个使用微服务部署的云原生应用程序都需要服务网格。事实上，很多应用程序并不需要服务网格，为它们添加服务网格实际上可能弊大于利。在决定服务网格是否适合特定情况时，您应该考虑以下重要因素。

+ 微服务的数量

  如果您的应用程序只有少量微服务，那么使用服务网格的好处就会很有限。比方说，您的应用程序有两个微服务。从它们现有的状态来看，服务网格在提高弹性和可观察性方面能做的并不多。

  随着应用程序中微服务数量的增加，应用程序的复杂性也越来越高，这就使得服务网格更有利于支持所有微服务及其相互关系之间的弹性、可观察性和安全性。

+ 微服务拓扑

  这指的是微服务被调用和相互调用的流程。在浅层拓扑中，微服务之间的直接交互不多；大多数请求来自集群外部。服务网格在浅层拓扑中通常没有太大价值，因为它们的主要目的是处理服务对服务的请求。

  在较深的微服务拓扑中，许多微服务会向其他微服务发送请求，而其他微服务又会向其他微服务发送请求，依此类推，服务网格在了解这些请求、确保通信安全以及添加弹性功能以防止级联故障和其他问题方面具有重要价值。

+ 排除网络问题

  尽管 Service Mesh 可能会带来延迟，而且这似乎会适得其反，但 Service Mesh 还是有助于排除 Kubernetes 集群中服务和 pod 之间的网络延迟问题。

如果您不确定在特定情况下是否需要服务网格，可以考虑仅使用 Ingress 控制器是否足够。Ingress 控制器和服务网格提供的功能有一些重叠，特别是在弹性和可观察性方面。

Ingress 控制器与服务网格的最大区别在于，在服务网格中，你可以控制微服务之间的所有通信；而在 Ingress 控制器中，你却不能。这意味着当你采用服务网格时，它将影响你的微服务和应用的开发。而使用 Ingress 控制器则不会有这种影响。

# 服务网格接口

我们将讨论服务网格接口（SMI）。SMI 是一个正在开发中的规范。它为 Kubernetes 集群上的服务网格定义了一套标准的应用编程接口（API）。SMI 的目标是支持通用服务网格功能的互操作性。

我们将：

+ 讨论服务网格接口 (SMI) 的目的。
+ 描述流量规格、流量分割、流量访问控制和Traffic Metrics APIs。
+ 解释如何一起使用不同的 SMI API。

## 服务网格接口（SMI）简介

服务网格接口（SMI）是一个正在积极开发中的 Kubernetes 原生规范。它为服务网格定义了一套标准的 Kubernetes 自定义资源定义（CRD）和 API。SMI 的开发过程是开放的，依赖于社区的参与，规范本身在 Apache 2.0 许可下公开发布。

正如 [SMI website](https://smi-spec.io/)所述，"SMI API 的目标是提供一套通用、可移植的服务网格 API，Kubernetes 用户可以不依赖提供商的方式使用这些 API。这样，人们就可以定义使用服务网格技术的应用程序，而无需与任何特定实现紧密绑定。SMI 并不包括服务网格实现；它只是一个服务网格实现可以使用的规范。

如果没有像 SMI 这样的标准，每种服务网格技术都会有完全不同的应用程序接口。这将使开发人员、平台和 DevOps 团队的工作变得更加复杂和耗时。开发人员将不得不编写自己的代码来提供同等功能，而不是使用 SMI API。与其他技术和工具集成将花费更多精力，应用程序使用哪种服务网格技术也会发生变化。

接下来将详细介绍四种 SMI API，它们有助于管理、保护和观察集群内的服务对服务流量：

- Traffic Specs
- Traffic Split
- Traffic Access Control
- Traffic Metrics

所有这些 API 都涉及服务网格概念，我们已经在前面的中进行了研究。我们将以这些知识为基础，重点举例说明这些 API 的支持内容和使用方法。

请注意，服务网格技术正在采用 SMI API。此外，SMI 本身也在快速发展，不断进行更新。在可预见的未来，常见的服务网格技术很可能在任何时候都会对每种 SMI API 提供不同程度的支持。

```
SMI 网站的网址是 https://smi-spec.io，其中包含有关 SMI 的基本信息。SMI 规范本身位于 GitHub 存储库中，网址是 https://github.com/servicemeshinterface/smi-spec。
```

### Traffic Specs API

流量规格 API 允许您定义应用程序可以使用的路由。没有示例很难解释这一点，因此我们来看一个示例来说明这意味着什么。

本例说明了如何使用流量规范 API 中的 HTTPRouteGroup 来定义名为 m-routes 的资源。它基于 SMI 规范中的一个示例。它将与应用程序看到的路径中包含字符串"/metrics "的任何 HTTP GET 请求相匹配。

HTTPRouteGroup 本身不起任何作用。它可以匹配，但不会以任何方式对匹配结果采取行动。HTTPRouteGroup 的目的是供其他 SMI API 使用，这些 API 可以执行任何操作。因此，举例来说，流量拆分 API 可以在自己的资源中引用该组，比如声明匹配 m-routes 定义的流量应在特定微服务的两个版本之间平均拆分。

```yaml
kind: HTTPRouteGroup
metadata:
  name: m-routes
spec:
  matches:
  - name: metrics
    pathRegex: "/metrics"
    methods:
    - GET
```

### Traffic Split API

通过流量分割 API，您可以实施流量分割和流量转移方法，如 A/B 测试、蓝绿部署和金丝雀部署。

下面是一个为金丝雀部署定义 TrafficSplit 的示例。在后端下有两个服务，分别名为：e8-widget-svc-current 和 e8-widget-svc-patch。这两个服务（实际上是微服务）将被路由到新请求。每个服务都有一个权重。这些权重看起来像百分比--75 和 25，加起来是 100，但实际上并不是百分比。你可以指定 750 和 250，或 3 和 1，或任何其他具有 3 比 1 比例的数字对，以实现相同的结果。

```yaml
kind: TrafficSplit
metadata:
  name: e8-feature-test
  namespace: e8app
spec:
  service: e8-widget-svc
  backends:
  - service: e8-widget-svc-current
    weight: 75
  - service: e8-widget-svc-patch
    weight: 25
```

随着时间的推移，为了继续金丝雀的部署，你会更新权重：例如，接下来你可能会将这些权重设置为 50 和 50。如果进展顺利，下一步可能是 25 和 75，最后一步是 0 和 100。

要使用 TrafficSplit 进行 A/B 测试，您需要为每个服务分配相同的权重。

在使用 TrafficSplit 进行蓝绿部署时，最初会将蓝色环境服务的权重设置为 100，将绿色环境服务的权重设置为 0。当准备从蓝色环境切换到绿色环境时，会交换 100 和 0 的权重，这样所有新请求都会路由到绿色环境服务。

如果要对三个或更多服务进行流量拆分，只需在后端下列出每个服务，并为每个服务分配一个权重即可。下面是一个例子，其中基线版本的服务获得了一半的请求，另外两个测试新功能的版本各获得了四分之一的请求：

```yaml
kind: TrafficSplit
metadata:
  name: e8-feature-test
  namespace: e8app
spec:
  service: e8-render-svc
  backends:
  - service: e8-render-svc-baseline
    weight: 50
  - service: e8-render-svc-feature1
    weight: 25
  - service: e8-render-svc-feature2
    weight: 25
```

### Traffic Access Control API

流量访问控制 API 允许您根据服务代理身份为 pod 到 pod（服务代理到服务代理）通信设置访问控制策略。使用此 API 时，默认情况下会拒绝所有流量。您必须为任何类型的流量明确授予权限。

下面是一个定义 TrafficTarget 的示例，以 SMI 规范中的示例为基础。在规范中，定义了三件事：

- **sources**, 指定可能成为流量来源的 pod。
- **destination**, 指定可作为流量目的地的 pod。
- **rules**, 定义了允许流量到达目的地所必须具备的特征。

```yaml
kind: TrafficTarget
metadata:
  name: path-specific
  namespace: default
spec:
  destination:
    kind: ServiceAccount
    name: service-a
    namespace: default
    port: 8080
  rules:
  - kind: HTTPRouteGroup
    name: m-routes
    matches:
    - metrics
  sources:
  - kind: ServiceAccount
  name: prometheus
  namespace: default
```

### Traffic Metrics API

流量度量 API 允许你收集 HTTP 流量指标，并将这些度量提供给其他工具。每个指标都涉及 Kubernetes 资源，可以是 pod 或service等低级资源，也可以是命名空间等高级资源。每个指标也仅限于特定的edge，edge是流量来源或目的地的另一种说法。请注意，edge可以设置为空白，这样就可以匹配所有流量。

下面是一个定义 TrafficMetrics 的示例，它基于 SMI 规范中的一个示例，定义了以下几个方面：

- **resource** 指定要收集指标的流量来源。
- **edge**, 指定要收集度量指标的流量目的地
- **timestamp**, 指定定义的创建时间。
- **window**, 指定用于计算指标的时间段。在本例中，指定的是 30 秒，因此将根据过去 30 秒的活动计算指标
- **metrics**,列出要收集的度量值。在本例中，指标将包括响应延迟、成功和失败请求的数据

```yaml
kind: TrafficMetrics
# See ObjectReference v1 core for full spec
resource:
  name: foo-775b9cbd88-ntxsl
  namespace: foobar
  kind: Pod
edge:
  direction: to
  side: client
  resource:
    name: baz-577db7d977-lsk2q
    namespace: foobar
    kind: Pod
timestamp: 2019-04-08T22:25:55Z
window: 30s
metrics:
- name: p99_response_latency
  unit: seconds
  value: 10m
- name: p90_response_latency
  unit: seconds
  value: 10m
- name: p50_response_latency
  unit: seconds
  value: 10m
- name: success_count
  value: 100
- name: failure_count
  value: 100
```

Traffic Metrics  API 目前还处于早期阶段，因此对单个指标的支持非常有限。

# 使用服务网格DEBUG和MITIGATE(缓解) 应用程序故障

在本章中，我们将讨论如何使用服务网格来调试和缓解某些类型的应用程序故障。我们将了解服务网格可能提供的几种功能。每种服务网格技术都支持一组独特的功能。

在本章的实验中，您将使用 Linkerd 防止某些类型的应用程序故障，查找演示应用程序的其他问题，并暂时缓解问题，以便开发人员有时间创建永久解决方案。为了让您为这些实验做好准备，本章将使用 Linkerd 中的示例来说明服务网格可能提供的功能，但本章中的基本概念适用于任何服务网格。

我们将：

+ 描述使用服务网格调试和缓解应用程序故障的几种技术。
+ 讨论 Linkerd 如何支持应用程序故障调试和缓解技术。

## Service Mesh Status Checks

在很多情况下，首先检查服务网格组件的状态会很有帮助。如果网状结构本身出现故障，例如控制平面无法工作，那么您看到的应用程序故障可能是由更大的问题引起的，而不是应用程序本身的问题。

+ 服务网格能否与 Kubernetes 通信？

  ([kubernetes-api checks](https://linkerd.io/2/tasks/troubleshooting/#k8s-api))

+ Kubernetes 版本与服务网格版本兼容吗？

  ([kubernetes-version checks](https://linkerd.io/2/tasks/troubleshooting/#k8s-version))

+ 服务网格是否已安装并正在运行？

  ([linkerd-existence checks](https://linkerd.io/2/tasks/troubleshooting/#l5d-existence))

+ 服务网格的控制平面是否配置正确？

  ([linkerd-config checks](https://linkerd.io/2/tasks/troubleshooting/#l5d-config))

+ 服务网格的证书是否有效和最新？

  ([linkerd-identity checks](https://linkerd.io/2/tasks/troubleshooting/#l5d-identity))

+ 控制平面的API是否运行就绪？

  ([linkerd-api checks](https://linkerd.io/2/tasks/troubleshooting/#l5d-api))

+ 服务网格的安装是否是最新的？

  ([linkerd-version checks](https://linkerd.io/2/tasks/troubleshooting/#l5d-version))

+ 服务网格控制平面是否是最新的？

  ([control-plane-version checks](https://trainingportal.linuxfoundation.org/learn/course/service-mesh-fundamentals-lfs243/using-service-mesh-to-debug-and-mitigate-app-failures/ADDWEBSITEADDRESS))

## Service Proxy Status Checks

有时，您可能想对服务网格的其他方面进行状态检查，而不是检查您刚刚检查过的方面。例如，您可能只想检查应用程序应该使用的服务代理的状态。在 Linkerd 中，您可以通过在 **linkerd check** 命令中添加 **--proxy **标志来实现这一点。

**请注意，Linkerd 将 "service proxies"称为 "data plane proxies."。**

Below is an excerpt of possible output from running **linkerd check --proxy**. This command runs all the same checks as the **linkerd check** command, plus a few additional ones specific to data planes. The duplication of the **linkerd check** output has been omitted here for brevity. The sample output shows the additional checks performed because of the **--proxy** flag.

下面是运行 **linkerd check --proxy** 可能产生的输出结果摘录。该命令运行与 **linkerd check** 命令相同的所有检查，另外还有一些专门针对数据平面的检查。为简洁起见，此处省略了重复的 **linkerd check** 输出。示例输出显示了由于 **--proxy** 标记而执行的额外检查。

+ 每个数据平面代理的凭证是否有效且是最新的？

  ([linkerd-identity-data-plane](https://linkerd.io/2/tasks/troubleshooting/#l5d-identity-data-plane))

+ 数据平面代理是否运行正常?

  ([linkerd-identity-data-plane](https://linkerd.io/2/tasks/troubleshooting/#l5d-identity-data-plane))

还有其他方法对 Linkerd 服务网格进行状态检查。有关 linkerd 检查命令及其用于执行不同检查的标记的更多信息，请参阅 https://linkerd.io/2/reference/cli/check/ 和 https://linkerd.io/2/tasks/troubleshooting/。

## Service Route Metrics

如果服务网格状态检查没有报告任何问题，常见的下一步故障排除方法是查看网格中应用程序服务路由的指标。换句话说，您要查看应用程序使用的网格内每条路由的性能测量值。这些测量值通常用于故障排除以外的其他用途，例如确定如何提高应用程序的性能。

假设您正在对一个出现间歇性减速和故障的应用程序进行故障排除。您可以查看该应用程序的每条路由指标，看看是否有特定路由是原因所在或以某种方式涉及其中。

**linkerd routes** 命令会返回特定范围内每条路由的性能指标快照。该范围由所谓的服务service profile（本章后面将详细介绍服务配置文件）。下面是该命令的输出头和一行示例指标的示例。为了提高可读性，我们对原始格式进行了调整。

```
ROUTE SERVICE   SUCCESS     RPS    LATENCY_P50 LATENCY_P95 LATENCY_P99
GET / webapp    100.00%    0.6rps        15ms     20ms           20ms
```

默认情况下，指标只针对入站请求。此示例显示了通过 "GET /"路由向 webapp 服务发出的请求的性能。在本例中，最近的请求 100% 成功，平均每秒处理 0.6 个请求。

三个延迟指标显示了根据百分位数处理请求所需的时间。P50 指的是第 50 个百分位数--中位时间，在本例中为 15 毫秒。P95 指的是第 95 个百分位数，表示应用程序处理 95% 的请求的速度等于或快于 20 毫秒。P99 为第 99 百分位数提供了相同类型的度量。

查看每条路由的度量值可以显示网状结构中哪些地方出现了速度减慢或故障，哪些地方运行正常。它可以缩小问题的范围，或在某些情况下直接指出罪魁祸首。

## Service Route Configuration 以缓解所在问题

您可能需要更改特定服务路由的配置，以减轻发生的问题。例如，您可以更改特定路由的超时和自动重试，从而使尝试到有问题 pod 的过程更快地切换到另一个 pod。在您继续排除故障或开发人员修改代码以解决根本问题时，这可以减少用户的延迟。

在 Linkerd 中，配置路由的机制称为**service profile**。如本章前面所述，服务配置文件还可用于为 linkerd routes 命令指定提供指标的路由。

## Request Logging

如果需要比服务路由指标更详细的请求和响应信息，可能需要对单个请求和响应进行日志记录。

**注意：** 记录请求会产生快速增长的日志数据。在很多情况下，你只需要查看几个日志请求，而不是大量的请求。

下面是几个日志请求的示例。该日志是通过运行 linkerd tap 命令生成的。为提高可读性，日志条目之间添加了空行。这三个条目都涉及同一个请求。

第一个条目显示了请求内容，第二个条目显示了返回的状态代码（本例中为 503，服务不可用）。第二和第三个条目都包含如何处理该请求的指标。除了在route-level metrics中可以看到的信息外，这些附加信息可以帮助您缩小查找问题的范围。

```
req id=9:49 proxy=out src=10.244.0.53:37820 dst=10.244.0.50:7001 tls=true :method=HEAD :authority=authors:7001 :path=/authors/3252.json

rsp id=9:49 proxy=out src=10.244.0.53:37820 dst=10.244.0.50:7001 tls=true :status=503 latency=2197µs      

end id=9:49 proxy=out src=10.244.0.53:37820 dst=10.244.0.50:7001 tls=true duration=16µs response-length=0B           
```

For more information on the **linkerd tap** command, see [htt‌ps://linkerd.io/2/reference/cli/tap/](https://linkerd.io/2/reference/cli/tap/).

## Service Proxy Logging

有时，您想更好地了解特定服务代理中发生的情况。为此，您可以增加服务代理执行日志记录的范围，例如记录更多事件或记录每个事件的更多细节。

在更改服务代理日志之前要非常小心，因为这会对代理的性能产生负面影响，而且日志本身的数量也会让人难以承受。

Linkerd 允许以各种方式更改其服务代理日志级别。更多信息，请参阅 https://linkerd.io/2/tasks/modifying-proxy-log-level/ 和 https://linkerd.io/2/reference/proxy-log-level/。

以下是 Linkerd 服务代理日志的示例。

```
[ 326.996211471s] WARN inbound:accept{peer.addr=10.244.0.111:55288}:source{target.addr=10.244.0.131:7002}:http1{name=books.booksapp.svc.cluster.local:7002 port=7002 keep_alive=true wants_h1_upgrade=false was_absolute_form=false}:profile{addr=books.booksapp.svc.cluster.local:7002}:daemon:poll_profile: linkerd2_service_profiles::client: Could not fetch profile error=grpc-status: Unavailable, grpc-message: "proxy max-concurrency exhausted"
```

## Injecting a Debug Container

如果需要更仔细地查看 pod 内发生的情况，可以让服务网格向 pod 注入一个调试容器。调试容器旨在监控 pod 内的活动并收集活动信息，例如捕获网络数据包。

在 Linkerd 中，你可以通过在 linkerd inject 命令中添加 --enable-debug-sidecar 标志来注入调试容器。然后，您可以打开调试容器的 shell，并在容器内发布命令，以收集更多信息并继续排除故障。

有关使用 Linkerd 调试容器（称为 "debug sidecar"）的更多信息，请参阅 https://linkerd.io/2/tasks/using-the-debug-container/。

## Using the Telepresence Tool

[Telepresence](https://www.telepresence.io/)是由云原生计算基金会 (CNCF) 托管的调试工具。它可以用来代替或补充注入调试容器，以便检查 pod 内部发生的情况。

使用Telepresence，您可以在本地运行单个进程（服务或调试工具），双向网络代理可使本地服务作为远程 Kubernetes 集群的一部分有效运行。这种架构意味着Telepresence通常不会在生产集群中运行，而是用于测试或暂存。

由于您是在本地运行服务，因此您可以使用任何调试或测试工具来监控和探测正在执行的服务。您还可以使用任何工具编辑服务。

